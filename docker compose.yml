# По умолчанию: только postgres для локального фронта (npm run web).
# С краулерами: docker compose -f "docker compose.yml" --profile crawler up -d
services:
    kafka:
        image: confluentinc/cp-kafka:7.5.0
        profiles:
            - crawler
        ports:
            - '9092:9092'
        environment:
            CLUSTER_ID: MkU3OEVBNTcwNTJENDM2Qk
            KAFKA_NODE_ID: 1
            KAFKA_PROCESS_ROLES: broker,controller
            KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
            KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT
            KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
            KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
            KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
            KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
        volumes:
            - kafka_data:/var/lib/kafka/data

    kafka-init:
        image: confluentinc/cp-kafka:7.5.0
        profiles:
            - crawler
        depends_on:
            - kafka
        entrypoint: ['/bin/sh', '-c']
        command:
            - |
                sleep 10
                kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic crawl-urls --partitions 6 --replication-factor 1
                kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic crawl-urls-priority --partitions 6 --replication-factor 1
        restart: 'no'

    redis:
        image: redis:7-alpine
        profiles:
            - crawler
        ports:
            - '6379:6379'
        command: redis-server --maxmemory 1gb --maxmemory-policy allkeys-lru

    postgres:
        image: postgres:16-alpine
        environment:
            POSTGRES_DB: index
            POSTGRES_USER: scraper
            POSTGRES_PASSWORD: scraperpass
        ports:
            - '5432:5432'
        volumes:
            - postgres_data:/var/lib/postgresql/data

    scraper:
        profiles:
            - crawler
        build: .
        restart: unless-stopped
        environment:
            - KAFKA_BROKERS=kafka:9092
            - REDIS_URL=redis://redis:6379
            - DATABASE_URL=postgresql://scraper:scraperpass@postgres:5432/index
            - METRICS_PORT=9090
            - SEED_URLS=https://en.wikipedia.org/wiki/Main_Page,https://en.wikipedia.org/wiki/Lists_of_websites,https://news.ycombinator.com,https://curlie.org,https://github.com/explore,https://www.reddit.com,https://stackoverflow.com,https://www.bbc.com,https://www.cnn.com,https://www.mozilla.org,https://www.python.org,https://nodejs.org,https://developer.mozilla.org,https://archive.org,https://stackexchange.com,https://www.reuters.com,https://www.theguardian.com,https://www.npr.org,https://arstechnica.com,https://www.theverge.com,https://www.wired.com,https://techcrunch.com,https://www.fandom.com,https://www.imdb.com,https://www.npmjs.com,https://gitlab.com
            - CRAWL_ALLOWED_TLDS=com,net,org,io,ru,de,fr,uk,edu,gov,co,info,biz
            - RATE_LIMIT_PER_SECOND=40
        depends_on:
            - kafka
            - redis
            - postgres
        deploy:
            resources:
                limits:
                    memory: 8G

    web:
        profiles:
            - crawler
        build:
            context: ./frontend
            dockerfile: Dockerfile
        restart: unless-stopped
        ports:
            - '3000:3000'
        environment:
            - DATABASE_URL=postgresql://scraper:scraperpass@postgres:5432/index
        depends_on:
            - postgres

    nginx:
        profiles:
            - crawler
        image: nginx:alpine
        ports:
            - '80:80'
        volumes:
            - ./nginx.conf:/etc/nginx/nginx.conf:ro
        depends_on:
            - web

volumes:
    postgres_data:
    kafka_data:
